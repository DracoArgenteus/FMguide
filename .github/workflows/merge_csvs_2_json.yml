name: Merge Event Data to JSON

# This action automatically merges multiple CSV event files into a single,
# web-ready JSON file. It ensures the data displayed on your site is
# always synchronized with your sources.

# Controls when the action will run.
on:
  # Triggers the workflow on push events but only for the main branch
  push:
    branches:
      - main
    # ***UPDATED***: The action now only triggers if one of these specific files is changed.
    paths:
      - 'data/official_events_combined.csv'
      - 'data/folkemoedet_private_events.csv'
      - 'data/social_events.csv'

  # Allows you to run this workflow manually from the Actions tab on GitHub
  workflow_dispatch:

jobs:
  merge-data:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checks out your repository under $GITHUB_WORKSPACE, so your job can access it
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment that our script will use
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Run the Python script to perform the merge and conversion
      # This script reads the CSVs, assigns a 'DataSource' to each, generates
      # a unique ID for each event, and writes the final JSON file.
      - name: Merge CSVs and convert to JSON
        run: |
          # The Python script is defined inline here for simplicity.
          # In larger projects, you would typically keep this as a separate .py file
          # in your repository, for example in a '.github/scripts/' directory.
          cat <<'EOF' > merge_script.py
import csv
import json
import hashlib
import os

def generate_id(row, row_num, source):
    """Generates a unique and stable ID for an event row."""
    # Use a combination of key fields to create a stable hash.
    # The source and row number are used as a fallback for uniqueness.
    key_string = f"{row.get('EventTitle', '')}{row.get('TimeRange', '')}{row.get('Location', '')}{source}{row_num}"
    return hashlib.md5(key_string.encode()).hexdigest()[:12]

def process_csv(file_path, data_source):
    """Reads a CSV file, adds a DataSource, and generates unique IDs."""
    events = []
    try:
        with open(file_path, mode='r', encoding='utf-8-sig') as infile:
            reader = csv.DictReader(infile)
            for i, row in enumerate(reader):
                # Assign the data source based on the file
                row['DataSource'] = data_source
                # Generate a unique and stable ID for each event
                row['id'] = generate_id(row, i, data_source)
                events.append(row)
    except FileNotFoundError:
        print(f"Warning: File not found at {file_path}. Skipping.")
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
    return events

def main():
    # Define the files to process and their corresponding data sources
    files_to_process = {
        'data/official_events_combined.csv': 'Official',
        'data/folkemoedet_private_events.csv': 'Private',
        'data/social_events.csv': 'Social'
    }

    all_events = []
    for file_path, data_source in files_to_process.items():
        all_events.extend(process_csv(file_path, data_source))

    # Define the output path for the web app
    output_dir = 'public'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    output_file = os.path.join(output_dir, 'events.json')

    # Write the combined data to a single JSON file
    with open(output_file, 'w', encoding='utf-8') as outfile:
        json.dump(all_events, outfile, indent=2, ensure_ascii=False)

    print(f"Successfully merged {len(all_events)} events into {output_file}")

if __name__ == "__main__":
    main()
EOF
          python merge_script.py

      # Step 4: Commit the generated JSON file back to the repository
      # This step only runs if the events.json file has actually changed.
      - name: Commit and push if changed
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add public/events.json
          # Check if there are changes to commit and push them to the main branch
          if git diff --staged --quiet; then
            echo "No changes to commit. events.json is already up-to-date."
          else
            git commit -m "Automated: Update public/events.json from CSV sources"
            git push
          fi
